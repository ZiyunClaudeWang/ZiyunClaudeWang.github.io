<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ziyun (Claude) Wang</title>


    <meta name="author" content="Ziyun Wang">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="shortcut icon" href="images/favicon.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-5TYPQ2VXYS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5TYPQ2VXYS');
</script>
  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Ziyun (Claude) Wang
                </p>
                <p>
                  I'm a Ph.D. student at the <a href="https://www.grasp.upenn.edu/">GRASP Laboratory</a> at the University of Pennsylvania. 
                  I'm advised by <a href="https://www.cis.upenn.edu/~kostas/">Professor Kostas Daniilidis</a>. 
                  I received my B.S. in Computer Science from Rice University and M.S.E in Robotics from Penn. 
                  Previously, I was a research intern at Samsung AI Center New York, where I worked with Professors <a href="https://pni.princeton.edu/people/h-sebastian-seung">Sebastian Seung</a>, <a href="https://tech.cornell.edu/people/daniel-d-lee-2/">Daniel Lee</a>, and <a href="https://cse.umn.edu/mnri/volkan-isler">Volkan Isler</a>.
                  I did my most recent internship at the Apple Vision Product team, working on the <a href="https://www.apple.com/apple-vision-pro/">Vision Pro</a>.
                </p>
                <!-- <p style="color: #f09228;"><strong>I am currently on the job market. Please reach out if you are interested!</strong></p> -->
                <p style="text-align:center">
                  <a href="mailto:ziyunw@seas.upenn.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/cv.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Wq7iaonvhawC&hl=en">Google Scholar</a> /&nbsp;
                  <a href="https://twitter.com/ZiyunClaudeWang">X</a> &nbsp;
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile_pic.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile_pic.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding: 10px; width: 100%; vertical-align: middle;">
                <h2>News & Updates</h2>
                <ul></ul>
                  <!-- <li>[01/22/2025] I g!</li> -->
                  <!-- <li>[01/22/2025] Our paper about equivariant neural IMU is accepted at <strong>ICLR 2025</strong>!</li> -->
                  <li>[03/30/2025] Our paper "Event-based Continuous Color Video Decompression" will appear in <strong>CVPR 2025 Workshop on Event-based Vision</strong>.</li>
                  <!-- <li>[03/11/2025] I gave a talk at the Computer Science department at Stony Brook University.</li>
                  <li>[03/04/2025] I gave a talk at the ECE department at Johns Hopkins University.</li> -->
                  <li>[01/22/2025] Our paper about equivariant neural IMU is accepted at <strong>ICLR 2025</strong>!</li>
                  <li>[01/16/2025] I will give a lightning talk about event-based human motion field at <strong>NYC Computer Vision Day 2025</strong>. </li>
                  <!-- <li>[01/10/2025] I will co-organize event-based visual odometry challenge at 5th International Workshop on Event-Based Vision (CVPR 2025).</li> -->
                  <li>[09/30/2024] I am selected as one of the <strong>outstanding reviewers</strong> for ECCV 2024.</li>
                  <li>[08/23/2024] I finished my internship with the Vision Product Group at Apple.</li>
                  <li>[07/03/2024] <strong>Four papers </strong>  accepted to ECCV 2024. See you in Milan!</li>
                  <li>[06/18/2023] Our new dataset M3ED is presented at the CVPR event vision workshop.</li>
                  <li>[07/12/2022] "EV-Catcher: High-Speed Object Catching Using Low-latency Event-based Neural Networks" is accepted at RA-L.</li>
                  <li>[07/08/2022] EvAC3D: From Event-Based Apparent Contours to 3D Models via Continuous Visual Hulls" is selected as <strong>Oral Presentation </strong>at ECCV 2022. See you in Tel Aviv!</strong></li>
                </ul>
              </td>
            </tr>
              <!-- <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>News</h2>
              <ul>
                <li>[08/23/2024] I finished my internship with the Vison Product Group at Apple.</li>
                <li>[07/03/2024] We have 4 papers accepted to ECCV 2024. See you in Milan!</li>
                <li>[06/18/2023] Our new dataset M3ED is presented at the CVPR event vision workshop.</li>
                <li>[07/12/2022] "EV-Catcher: High Speed Object Catching Using Low-latency Event-based
                  Neural Networks" is accpeted at RA-L.</li>
                <li>[07/08/2022] "EV-Catcher: High Speed Object Catching Using Low-latency Event-based
                  Neural Networks" paper is selected as Oral Presentation at ECCV 2022. See you in Tel Aviv!</li>
              </ul>
              </td>
            </tr> -->
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My main research interests are event-based vision, 3D computer vision and robotics.
                  In particular, I work on motion and scene understanding of highly dynamic scenes. 
                  My research projects range from fundamental geometry problems in event-based vision to applications of the event sensors in real robots.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src="images/eqnio.png" width=160>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a>
              <span class="papertitle"> EqNIO: Subequivariant Neural Inertial Odometry <Title></Title></span>
            </a>
            <br>
            <a> Royina Karegoudra Jayanth*</a>,
            <a> Yinshuang Xu*</a>,
            <a> Daniel Gehrig</a>,
            <strong>Ziyun Wang</strong>,
            <a>Evangelos Chatzipantazis</a>,
            <a>Daniel Gehrig</a>,
            <a>Kostas Daniilidis</a>
            <br>
            International Conference on Learning Representations (ICLR) 2025
            <br>
            <!-- <a href="http://jonbarron.info/zipnerf">project page</a>
            /
            <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a>
            /
            <a href="https://arxiv.org/abs/">arXiv</a> -->
            <!-- <a href="https://www.cis.upenn.edu/~ziyunw/un_evmoseg/">project page</a> / -->
            <a href="https://arxiv.org/abs/2408.06321">Paper</a>
            <p></p>
            <p>
            <!-- Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x. -->
            </p>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src="images/unseg.gif" width=160>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a>
              <span class="papertitle">  Un-EVIMO: Unsupervised Event-based Independent Motion Segmentation <Title></Title></span>
            </a>
            <br>
            <strong>Ziyun Wang</strong>,
            <a>Jinyuan Guo</a>,
            <a>Kostas Daniilidis</a>
            <br>
            European Conference on Computer Vision (ECCV), 2024 
            <br>
            <!-- <a href="http://jonbarron.info/zipnerf">project page</a>
            /
            <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a>
            /
            <a href="https://arxiv.org/abs/">arXiv</a> -->
            <a href="https://www.cis.upenn.edu/~ziyunw/un_evmoseg/">project page</a> /
            <a href="https://arxiv.org/abs/2312.00114">Paper</a>
            <p></p>
            <p>
            <!-- Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x. -->
            </p>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src="images/motion_prior.png" width=160>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a>
              <span class="papertitle"> Motion-prior Contrast Maximization for Dense Continuous-Time Motion Estimation <Title></Title></span>
            </a>
            <br>
            <a> Friedhelm Hamman</a>,
            <strong>Ziyun Wang</strong>,
            <a>Ioannis Asmanis</a>,
            <a>Kenneth Chaney</a>,
            <a>Guillermo Gallego</a>,
            <a>Kostas Daniilidis</a>
            <br>
            European Conference on Computer Vision (ECCV), 2024 
            <br>
            <a href="https://github.com/tub-rip/MotionPriorCMax">project page</a> /
            <a href="https://arxiv.org/abs/2407.10802">Paper</a>
            <p></p>
            <p>
            </p>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <!-- <div>
              <img src="images/track_fast.gif" width=160>
            </div> -->
            <div class="col-md-4" style="overflow: hidden; position: relative;max-width: 100%;" >
              <video autoplay loop muted style="max-width: 100%;height: auto;">
                <source src='images/tram.mp4' type="video/mp4">
              </video>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a>
              <span class="papertitle"> TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Videos <Title></Title></span>
            </a>
            <br>
            <a>Yufu Wang</a>,
            <strong>Ziyun Wang</strong>,
            <a>Lingjie Liu</a>,
            <a>Kostas Daniilidis</a>
            <br>
            European Conference on Computer Vision (ECCV), 2024 
            <br>
            <a href="https://yufu-wang.github.io/tram4d/">project page</a> /
            <a href="https://arxiv.org/abs/2403.17346">Paper</a>
            <p></p>
            <p>
            </p>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src="images/track_fast.gif" width=160>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a>
              <span class="papertitle">  Track Everything Everywhere Fast and Robustly <Title></Title></span>
            </a>
            <br>
            <a>Yunzhou Song*</a>,
            <a>JiahuiLei*</a>,
            <strong>Ziyun Wang</strong>,
            <a>Lingjie Liu</a>,
            <a>Kostas Daniilidis</a>
            <br>
            European Conference on Computer Vision (ECCV), 2024 
            <br>
            <a href="https://timsong412.github.io/FastOmniTrack/">project page</a> /
            <a href="https://arxiv.org/abs/2403.17931">Paper</a>
            <p></p>
            <p>
            </p>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src="images/continuity_cam.gif" width=160>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a>
              <span class="papertitle">  Event-based Continuous Color Video Decompression from Single Frames <Title></Title></span>
            </a>
            <br>
            <strong>Ziyun Wang</strong>,
            <a>Friedhelm Hamann</a>,
            <a>Kenneth Chaney</a>,
            <a>Wan Jiang</a>,
            <a>Guillermo Gallego</a>,
            <a>Kostas Daniilidis</a>
            <br>
            Conference on Computer Vision and Pattern Recognition (CVPR), Event-based Vision Workshop 2025
            <br>
            <a href="https://www.cis.upenn.edu/~ziyunw/continuity_cam/">project page</a> /
            <a href="https://arxiv.org/abs/2312.00113">Paper</a>
            <p></p>
            <p>
            </p>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src="images/m3ed.gif" width=160 >
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a>
              <span class="papertitle"> M3ED: Multi-Robot, Multi-Sensor, Multi-Environment Event Dataset  <Title></Title></span>
            </a>
            <br>
            <a>Kenneth Channey*</a>,
            <a>Fernando Cladera*</a>,
            <strong>Ziyun Wang</strong>,
            <a>Anthony Bisulco</a>,
            <a>M Ani Hsieh</a>,
            <a>Christopher Korpela</a>,
            <a>Vijay Kumar</a>,
            <a>Camillo J Taylor</a>,
            <a>Kostas Daniilidis</a>
            <br>
            Event-based Vision Workshop, CVPR 2023
            <br>
            <a href="https://m3ed.io/">project page</a> /
            <a href="https://openaccess.thecvf.com/content/CVPR2023W/EventVision/papers/Chaney_M3ED_Multi-Robot_Multi-Sensor_Multi-Environment_Event_Dataset_CVPRW_2023_paper.pdf">paper</a>
            <p></p>
            <p>
            </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src="images/evac3d.gif" width=160>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a>
              <span class="papertitle"> EvAC3D: From Event-Based Apparent Contours to 3D Models via Continuous Visual Hulls  <Title></Title></span>
            </a>
            <br>
            <strong>Ziyun Wang*</strong>,
            <a>Kenneth Channey*</a>,
            <a>Kostas Daniilidis</a>
            <br>
            European Conference on Computer Vision (ECCV), 2022 <font color="red">(Oral Presentation, 2.7% Top Papers)</font>
            <br>
            <a href="https://www.cis.upenn.edu/~ziyunw/evac3d/">project page</a> /
            <a href="https://arxiv.org/abs/2304.05296">Paper</a>
            <p></p>
            <p>
            <!-- Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x. -->
            </p>
          </td>
        </tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <video  width=160 muted autoplay loop>
              <source src="images/ev_catcher.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video>
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a>
              <span class="papertitle"> EV-Catcher: High-Speed Object Catching Using Low-Latency Event-Based Neural Networks <Title></Title></span>
            </a>
            <br>
            <strong>Ziyun Wang*</strong>,
            <a>Fernando Cladera*</a>,
            <a>Anthony Bisculco</a>,
            <a>Daewon Lee</a>,
            <a>Camillo J Taylor</a>,
            <a>Kostas Daniilidis</a>,
            <a> M Ani Hsieh </a>,
            <a>Daniel D Lee</a> , 
            <a>Volkan Isler</a>
            <br>
            IEEE Robotics and Automation Letters (RA-L), 2022
            <br>
            <!-- <a href="https://www.cis.upenn.edu/~ziyunw/continuity_cam/">project page</a> -->
            <a href="https://arxiv.org/abs/2304.07200">Paper</a>
            <p></p>
            <p>
            <!-- Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x. -->
            </p>
          </td>
        </tr>
      <tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <img src='images/event_gan_000049.jpeg' width="160">
              <img src='images/event_gan_output_000049.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a>
              <span class="papertitle"> Eventgan: Leveraging large scale image datasets for event cameras <Title></Title></span>
            </a>
            <br>
            <a>Alex Zhu</a>,
            <strong>Ziyun Wang</strong>,
            <a>Kaung Khant</a>,
            <a>Kostas Daniilidis</a>,
            <br>
            IEEE International Conference on Computational Photography (ICCP), 2021
            <br>
            <a href="https://arxiv.org/abs/1912.01584">Paper</a>
            <p></p>
            <p>
            <!-- Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x. -->
            </p>
          </td>
        </tr>
      <tr>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div>
              <!-- <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop> -->
              <img src='images/cost_to_go_single.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a>
              <span class="papertitle"> Learning to generate cost-to-go functions for efficient motion planning <Title></Title></span>
            </a>
            <br>
            <a>Jinwook Huh</a>,
            <a>Galen Xing</a>,
            <strong>Ziyun Wang</strong>,
            <a>Volkan Isler</a>,
            <a>Daniel D. Lee</a>
            <br>
            Experimental Robotics: The 17th International Symposium, 2021
            <br>
            <a href="https://arxiv.org/abs/2010.14597">Paper</a>
            <p></p>
            <p>
            <!-- Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x. -->
            </p>
          </td>
        </tr>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div>
            <img src='images/surface_hof_dragon.png' width="160">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a>
            <span class="papertitle"> Surface HOF: Surface Reconstruction from a Single Image Using Higher Order Function Networks <Title></Title></span>
          </a>
          <br>
          <strong>Ziyun Wang</strong>,
          <a>Volkan Isler</a>,
          <a>Daniel D. Lee</a>
          <br>
          IEEE International Conference on Image Processing (ICIP), 2020
          <br>
          <a href="https://arxiv.org/abs/1912.08852">Paper</a>
          <p></p>
          <p>
          <!-- Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x. -->
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div>
            <img src='images/geodesic_plane.png' width="160">
            <img src='images/geodesic_cube.png' width="160">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a>
            <span class="papertitle"> Geodesic-HOF: 3D Reconstruction Without Cutting Corners <Title></Title></span>
          </a>
          <br>
          <strong>Ziyun Wang</strong>,
          <a>Eric Mitchell</a>,
          <a>Volkan Isler</a>,
          <a>Daniel D. Lee</a>
          <br>
          AAAI Conference on Artificial Intelligence, 2021
          <br>
          <a href="https://arxiv.org/abs/2006.07981">Paper</a>
          <p></p>
          <p>
          <!-- Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x. -->
          </p>
        </td>
      </tr>
    
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div>
            <img src='images/motion_equivariance.png' width="160">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a>
            <span class="papertitle"> Motion Equivariant Networks for Event Cameras with the Temporal Normalization Transform <Title></Title></span>
          </a>
          <br>
          <a>Alex Zhu</a>,
          <strong>Ziyun Wang</strong>,
          <a>Kostas Daniilidis</a>
          <br>
          <br>
          <a href="https://arxiv.org/abs/1902.06820">Paper</a>
          <p></p>
          <p>
          <!-- Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x. -->
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle;">
          <div>
            <img src='images/robust.png' width="160">
          </div>
        </td>

        <td style="padding:20px;width:75%;vertical-align:middle">
          <a>
            <span class="papertitle"> Robustness Meets Deep Learning: An End-to-End Hybrid Pipeline for Unsupervised Learning of Egomotion <Title></Title></span>
          </a>
          <br>
          <a>Alex Zhu</a>,
          <a>Wenxin Liu</a>,
          <strong>Ziyun Wang</strong>,
          <a>Vijay Kumar</a>,
          <a>Kostas Daniilidis</a>
          <br>
          Workshop on Deep Learning for Semantic Visual Navigation, CVPR 2019
          <br>
          <a href="https://arxiv.org/abs/1812.08351">Paper</a>
          <p></p>
          <p>
          <!-- Combining mip-NeRF 360 and grid-based models like Instant NGP lets us reduce error rates by 8%&ndash;77% and accelerate training by 24x. -->
          </p>
        </td>
      </tr>
    </table>
  </body>
</html>
